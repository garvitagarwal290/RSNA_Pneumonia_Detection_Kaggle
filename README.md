# Description

This is my implementation of a pneumonia detection model for X-ray scans of the lungs. It is based on Kaggle's [RSNA Pneumonia Detection Challenge](https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/overview) competition. My implementation takes inspiration from the discussions among the winners of the competition [here](https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/discussion?sort=hotness).

# Details

1) I used this [Retinanet detection model](https://github.com/yhenon/pytorch-retinanet) PyTorch implementation to start with.

2) modifications: augmentation, ensemble, dropout in classification

3) Training details: batchsize, optimiser, image size, number of epochs

4) Inference details: detection thresholds


(put create_csv code, training and prediction code in separate files to be run as separate commands and not in notebook)
## Training

1) Create csv annotation file. Use the command:
   > python create_trainingcsv.py

3) Start training with command:
   > python train.py --csv_train train_annots.csv --csv_classes class_list.csv  --csv_val val_annots.csv --epochs 4 --batchsize 32

The trained models will be saved in the current directory.


## Prediction

1) Create csv annotation file. Use the command: > python create_inferencecsv.py

2) Start training with command: > python csv_prediction.py --csv_classes class_list.csv --csv_test test_annots.csv --model model_final.pt --num_images_topredict 3000 --score_threshold 0.05

The csv file containing the predictions will be created in the current directory.


## Visualise

You can visualize the model's output by running: > python visualize.py --csv_classes class_list.csv --csv_val test_annots.csv --model model_final.pt --score_threshold 0.05


Taking the option with the highest score as the model's `answer', the above model answered only 63 questions correctly out of 200. This accuracy is not significantly more than a model that chooses an option randomly out of the 5 options. The sub-par performance might be due to the following reasons:

1) **_Limited GPU resources:_** All the above steps were implemented on Kaggle and so were subjected to the GPU memory and time limitations. This limited the size of the answering/reader model.
2) **_Limited context length:_** The maximum context length of our reader model was 512 tokens. This is likely not enough to accommodate many long Wikipedia articles which are hence truncated. The truncation might result in the loss of essential context required to answer the question. Hence an encoder model with a larger context length should perform better, however, at the same time, the limited GPU memory might cause a roadblock.
3) **_Limited Wikipedia dataset:_** Though our Wikipedia dataset had 130k articles, the test questions are supposed to be difficult questions generated by GPT3.5. It was seen that for many test questions, the Wikipedia article identified by our retrieval model did not actually have the information required to answer the question. Hence a bigger retrieval dataset should improve performance.

To conclude, more work is required to build a more accurate RAG-based MCQ answering model.

